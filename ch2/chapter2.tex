% Chapter 2

\chapter{State of the art} % Main chapter title

\label{chap:Chapter2} % For referencing the chapter elsewhere, use \ref{chap:Chapter2} 

%----------------------------------------------------------------------------------------

For the \textit{State of the art} chapter, an extensive examination of studies related to multiclass imbalance in machine learning was conducted. From this review four topics of significance were identified, each worthy of a dedicated section. Each section provides an introduction to the topic and explores its relevance to multiclass imbalance training.

The first section presents a general review of the literature to explore the impact and reach of multi-class imbalance in real world scenarios. The second section addresses Class Imbalance problems, detailing how imbalance-related challenges affect predictive accuracy and discussing various balancing strategies and techniques designed to mitigate these issues. The third section focuses on classification itself, explaining how models perform classification tasks, the main types of classification algorithms, and the algorithms most commonly applied in multiclass settings. The final section examines evaluation metrics, emphasizing the importance of the confusion matrix and discussing how multiple metrics should be applied to accurately interpret the results of models trained on multiclass imbalanced datasets.

\section{Review of the Literature}

% Give examples in professional context on the impact of the issue

% Talk about overfitting and underfitting


\section{Class Imbalance problems}

%  give an introduction to the three main balancing approaches: algorithmic approach, data preprocessing approach, feature selection approach. Show table of each approach type and some exemple methods. Talk about techniques of balancing like Over or Underssampling.

With the rapid expansion of artificial intelligence applications in recent years, the use of machine learning models has become widespread across multiple professional domains. As these applications diversify, the datasets used in each area vary significantly in composition, quality, and distribution. In many cases, certain events or classes occur far less frequently than others, creating disproportionate class representation. For instance, in bioengineering analysis, a dataset may predominantly represent a “Healthy” class, while only a few samples correspond to a specific condition such as “Heart condition.” In such scenarios, machine learning models are often most valuable when they can accurately identify and classify these underrepresented cases, which makes their recognition essential for achieving meaningful results.

Class imbalance refers to this unequal representation of classes within a dataset. The class containing the largest number of samples is known as the \textit{majority class}, while the less frequent ones are referred to as \textit{minority class}. The imbalance may range from mild, where the difference in representation is moderate, to severe, where the majority class overwhelmingly dominates the dataset.

When training classification algorithms, the greater the imbalance, the more difficult it becomes for the model to learn the characteristics of minority classes. Classifiers inherently tend to favor the majority class, leading to biased predictions and reduced accuracy for the minority classes. As a result, the model may fail to correctly identify rare cases, instead assigning most predictions to the majority class. This behavior is particularly problematic in fields where minority classes represent the most critical scenarios, such as the aforementioned bioengineering datasets, where the accurate detection of these rare events such as identifying anomalies or diseases is often the primary objective.

To ensure that classification models produce accurate and unbiased results, various balancing methods have been developed to modify dataset distributions by adding, removing, or adjusting samples. These techniques aim to achieve a more uniform representation of all classes, allowing models to better learn the characteristics of minority classes and improve overall predictive performance. Figure \ref{fig:imbalance-learning-techniques}.  illustrates the different balancing approaches discussed in the following subsections.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{ch2/assets/Taxonomy-of-imbalance-learning-techniques.png}
    \caption[Taxonomy-of-imbalance-learning-techniques]{Imbalanced Learning Methods.}
    \label{fig:imbalance-learning-techniques}
\end{figure}

\subsection{Data Level Methods}

Data-level methods focus on techniques designed to modify the dataset distribution either by increasing the number of samples in minority classes, referred to as oversampling, or by reducing the number of samples in majority classes, referred to as under sampling. Both approaches aim to achieve a more balanced dataset that allows classification algorithms to better learn the characteristics of all classes. The following subsections will describe each method in detail, outlining their underlying mechanisms, advantages, and potential limitations, as well as combined methods that use both over and under sampling to achieve balanced class representation.

\subsubsection{Over Sampling}

%Explain how it works, give examples, mention smore, adasyn and other methods, explain positives, negatives, mention possible overfitting

Over Sampling refers to the method of adding more samples of minority classes in order to achieve equal representation between classes and allow for classification algorithms that tend to be bias to majority classes to properly identify minority classes. 

One method of over sampling is random oversampling, where minority classes are ramdomly duplicated without any alteration to it until the dataset is balanced.
Another method is S


\subsubsection{Under Sampling}

%Explain how it works, give examples

\subsubsection{Combined Methods}

\section{Classification}

% introduction of the idea of classification, mentions the existance of different types of classifications depending on dataset, include explanation for binary classifications, multi-class for single label, and multi-class for multiple labels. Show graphs of both how classification works and how different classification relate to each other. Mention that single class won't be the focus of the dissertation.

\subsection{Multi-Class Single Label Classification}

% Explain how this works, mention examples and how they can be applied, show what situations can be applicable here.

% talk about neural network, one versus all, mention softmax in those cases, explain why its' specific to single label (assumes just one label)

% expand on neural network, naive, svm and decision tree for multi class classification

\subsection{Multi-Class Multiple Label Classification}

% Explain how this works, mention examples and how they can be applied, show what situations can be applicable here.

% show and explain how reworked versions of common algorithmns can be used for multi label

\section{Meta-Learning}

\section{Metrics}
