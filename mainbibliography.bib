@article{chen2024survey,
  author       = {Chen, W. and Yang, K. and Yu, Z. and Shi, Y. and Chen, C. P.},
  title        = {A survey on imbalanced learning: latest research, applications and future directions},
  journal      = {Artificial Intelligence Review},
  year         = {2024},
  volume       = {57},
  number       = {6},
  pages        = {137},
}

@article{lango2022multiclass,
  author       = {Lango, M. and Stefanowski, J.},
  title        = {What makes multi-class imbalanced problems difficult? An experimental study},
  journal      = {Expert Systems with Applications},
  year         = {2022},
  volume       = {199},
  pages        = {116962},
}

@article{dediego2022gpscore,
  author       = {De Diego, I. M. and Redondo, A. R. and Fernández, R. R. and Navarro, J. and Moguerza, J. M.},
  title        = {General Performance Score for classification problems},
  journal      = {Applied Intelligence},
  year         = {2022},
  volume       = {52},
  number       = {10},
  pages        = {12049--12063},
  doi          = {10.1007/s10489-021-03041-7},
}

@misc{vieira2025github,
  author       = {Vieira, P.},
  title        = {Automated Imbalanced Classification},
  howpublished = {\url{https://github.com/PedroVieira1160634/automated-imbalanced-classification}},
  note         = {Accessed on October 5, 2025},
}

@inproceedings{sarvani2023smoteadaboost,
  author       = {Sarvani, A. and Reddy, Y. S. and Reddy, Y. M. and Vijaya, R. and Lavanya, K.},
  title        = {A multi-level optimized strategy for imbalanced data classification based on SMOTE and AdaBoost},
  booktitle    = {Proceedings of Data Analytics and Management},
  year         = {2023},
  pages        = {223--238},
  publisher    = {Springer},
  address      = {Singapore},
  doi          = {10.1007/978-981-99-6550-2_18},
}

@article{haixiang2017review,
  author       = {Haixiang, G. and Yijing, L. and Shang, J. and Mingyun, G. and Yuanyue, H. and Bing, G.},
  title        = {Learning from class-imbalanced data: review of methods and applications},
  journal      = {Expert Systems with Applications},
  year         = {2017},
  volume       = {73},
  pages        = {220--239},
  doi          = {10.1016/j.eswa.2016.12.035},
}

@misc{dua2025uci,
  author       = {Dua, D. and Graff, C.},
  title        = {UCI Machine Learning Repository},
  howpublished = {\url{https://archive.ics.uci.edu/datasets}},
  note         = {University of California, Irvine, School of Information and Computer Sciences. Accessed on October 5, 2025},
}

@misc{alcala2025keel,
  author       = {Alcal{\'a}-Fdez, J. and Fern{\'a}ndez, A. and Luengo, J. and Derrac, J. and Garc{\'i}a, S. and S{\'a}nchez, L. and Herrera, F.},
  title        = {KEEL Dataset Repository for Data Mining with Imbalanced Data Sets},
  howpublished = {\url{https://sci2s.ugr.es/keel/datasets.php}},
  note         = {University of Granada. Accessed on October 5, 2025},
}

@misc{openml2025backend,
  author       = {{OpenML}},
  title        = {OpenML Dataset Contribution and Backend Documentation},
  howpublished = {\url{https://docs.openml.org/contributing/backend/Datasets/}},
  note         = {Accessed on October 5, 2025},
}

@misc{tpot2025,
  author       = {{TPOT AutoML}},
  title        = {TPOT --- AutoML},
  howpublished = {\url{https://automl.info/tpot/}},
  note         = {Accessed on October 7, 2025},
}

@online{olamendy2024practical,
  author       = {Olamendy, Juan C.},
  title        = {Practical ML: Addressing Class Imbalance},
  year         = {2024},
  url          = {https://medium.com/@juanc.olamendy/practical-ml-addressing-class-imbalance-25c4f1b97ee3},
  note         = {Medium article, accessed November 1, 2025}
}

@article{Zhang2023,
  author    = {Xinmin Zhang and Saite Fan and Zhihuan Song},
  title     = {Reinforcement learning-based cost-sensitive classifier for imbalanced fault classification},
  journal   = {Science China Information Sciences},
  year      = {2023},
  volume    = {66},
  pages     = {212201},
  doi       = {10.1007/s11432-021-3775-4}
}

@article{Vieira2024,
  author    = {Pedro Marques Vieira and Fátima Rodrigues},
  title     = {An automated approach for binary classification on imbalanced data},
  journal   = {Knowledge and Information Systems},
  year      = {2024},
  volume    = {66},
  pages     = {2747--2767},
  doi       = {10.1007/s10115-023-02046-7}
}

@article{Albattah2025,
  author    = {Waleed Albattah and Rehan Ullah Khan},
  title     = {Impact of imbalanced features on large datasets},
  journal   = {Frontiers in Big Data},
  year      = {2025},
  volume    = {8},
  doi       = {10.3389/fdata.2025.1455442},
  license   = {CC BY 4.0}
}

@online{Saltz2020,
  author    = {Jeff Saltz},
  title     = {CRISP-DM is still the most popular framework for executing data science projects},
  year      = {2020},
  month     = {August},
  url       = {https://www.datascience-pm.com/crisp-dm-still-most-popular/},
  note      = {Accessed: 2025-11-07},
  publisher = {Data Science Process Alliance}
}

@article{Shearer2000,
  author    = {Colin Shearer},
  title     = {The CRISP-DM Model: The New Blueprint for Data Mining},
  journal   = {Journal of Data Warehousing},
  year      = {2000},
  volume    = {5},
  number    = {4},
  pages     = {4--13},
  note      = {Fall 2000}
}

@online{DataSciencePM_CrispDM2,
  author    = {Data Science Process Alliance},
  title     = {CRISP-DM 2: How Popular is CRISP-DM},
  year      = {2024},
  url       = {https://www.datascience-pm.com/crisp-dm-2/#How_Popular_is_CRISP-DM},
  note      = {Accessed: 2025-11-07}
}

@online{ResearchGate2025,
  title     = {A class imbalance-aware review rating prediction using hybrid sampling and ensemble learning},
  note      = {Scientific Figure on ResearchGate: Taxonomy of imbalance learning techniques},
  year      = {2025},
  url       = {https://www.researchgate.net/figure/Taxonomy-of-imbalance-learning-techniques_fig1_344850955},
  note      = {Accessed: 2025-11-11}
}

@inproceedings{He2008,
  author    = {Haibo He and Yang Bai and Edwardo A. Garcia and Shutao Li},
  title     = {ADASYN: Adaptive Synthetic Sampling Approach for Imbalanced Learning},
  booktitle = {Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN 2008)},
  year      = {2008},
  pages     = {1322--1328},
  doi       = {10.1109/IJCNN.2008.4633969},
  month     = {June},
  address   = {Hong Kong}
}

@article{Japkowicz2002,
  author    = {Nathalie Japkowicz and Shaju Stephen},
  title     = {The class imbalance problem: A systematic study},
  journal   = {Intelligent Data Analysis: An International Journal},
  year      = {2002},
  volume    = {6},
  number    = {5},
  pages     = {429--449},
  doi       = {10.3233/IDA-2002-6504}
}

@online{MastersInDataScience_Undersampling,
  author    = {MastersInDataScience.org},
  title     = {What Is Undersampling?},
  year      = {2022},
  url       = {https://www.mastersindatascience.org/learning/statistics-data-science/undersampling/},
  note      = {Accessed: 2025-11-11}
}

@article{Rendon2020,
  author    = {Er{\'e}ndira Rend{\'o}n and Roberto Alejo and Carlos Castorena and Frank J. Isidro-Ortega and Everardo E. Granda-Guti{\'e}rrez},
  title     = {Data Sampling Methods to Deal With the Big Data Multi-Class Imbalance Problem},
  journal   = {Applied Sciences},
  year      = {2020},
  volume    = {10},
  number    = {4},
  pages     = {1276},
  doi       = {10.3390/app10041276}
}
@article{Tomek1976,
  author    = {Ivan Tomek},
  title     = {Two Modifications of CNN},
  journal   = {IEEE Transactions on Systems, Man, and Cybernetics},
  year      = {1976},
  volume    = {SMC-6},
  number    = {11},
  pages     = {769--772},
  doi       = {10.1109/TSMC.1976.4309523}
}

@article{Chawla2002,
  author    = {Nitesh V. Chawla and Kevin W. Bowyer and Lawrence O. Hall and W. Philip Kegelmeyer},
  title     = {SMOTE: Synthetic Minority Over-sampling Technique},
  journal   = {Journal of Artificial Intelligence Research},
  year      = {2002},
  volume    = {16},
  pages     = {321--357},
  doi       = {10.1613/jair.953}
}

@article{Ying_2019,
doi = {10.1088/1742-6596/1168/2/022022},
url = {https://doi.org/10.1088/1742-6596/1168/2/022022},
year = {2019},
month = {feb},
publisher = {IOP Publishing},
volume = {1168},
number = {2},
pages = {022022},
author = {Ying, Xue},
title = {An Overview of Overfitting and its Solutions},
journal = {Journal of Physics: Conference Series},
abstract = {Overfitting is a fundamental issue in supervised machine learning which prevents us from perfectly generalizing the models to well fit observed data on training data, as well as unseen data on testing set. Because of the presence of noise, the limited size of training set, and the complexity of classifiers, overfitting happens. This paper is going to talk about overfitting from the perspectives of causes and solutions. To reduce the effects of overfitting, various strategies are proposed to address to these causes: 1) “early-stopping” strategy is introduced to prevent overfitting by stopping training before the performance stops optimize; 2) “network-reduction” strategy is used to exclude the noises in training set; 3) “data-expansion” strategy is proposed for complicated models to fine-tune the hyper-parameters sets with a great amount of data; and 4) “regularization” strategy is proposed to guarantee models performance to a great extent while dealing with real world issues by feature-selection, and by distinguishing more useful and less useful features.}
}

@article{PEREIRA202095,
title = {MLTL: A multi-label approach for the Tomek Link undersampling algorithm},
journal = {Neurocomputing},
volume = {383},
pages = {95-105},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.11.076},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219316790},
author = {Rodolfo M. Pereira and Yandre M.G. Costa and Carlos N. {Silla Jr.}},
keywords = {Multi-label learning, Resampling techniques, Dataset imbalanceness},
abstract = {A large variety of problems are multi-labeled, which made the Multi-Label Classification field become an active topic in the machine learning community. However, real world problems tend to be imbalanced, meaning that some classes may have more samples than others. Learning from imbalanced datasets is a challenging task and for that has attracted the attention of researchers that have proposed some resampling algorithms to address this problem. This work presents two main contributions: A new resampling algorithm for multi-label classification problems named MLTL - Multi-Label Tomek Link, which is based on the standard Tomek Link resampling algorithm; A multi-label imbalanceness API for the Mulan framework. Results in seven well-known datasets showed that MLTL is a competitive technique when compared to other multi-label resampling methods from the literature.}
}

@article{wilson1972asymptotic,
  author    = {Wilson, D.},
  title     = {Asymptotic properties of nearest neighbor rules using edited data},
  journal   = {IEEE Transactions on Systems, Man, and Cybernetics},
  year      = {1972},
  volume    = {2},
  pages     = {408--420},
}

@article{hart1968condensed,
  title={The condensed nearest neighbor rule (corresp.)},
  author={Hart, Peter},
  journal={IEEE transactions on information theory},
  volume={14},
  number={3},
  pages={515--516},
  year={1968},
  publisher={IEEE}
}

@online{IBM2025_underfitting,
  author    = {IBM Corporation},
  title     = {What Is Underfitting?},
  year      = {2025},
  url       = {https://www.ibm.com/think/topics/underfitting},
  note      = {Accessed: 2025-11-12}
}

@article{XU2020103465,
title = {A hybrid sampling algorithm combining M-SMOTE and ENN based on Random forest for medical imbalanced data},
journal = {Journal of Biomedical Informatics},
volume = {107},
pages = {103465},
year = {2020},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2020.103465},
url = {https://www.sciencedirect.com/science/article/pii/S1532046420300940},
author = {Zhaozhao Xu and Derong Shen and Tiezheng Nie and Yue Kou},
keywords = {Medical diagnosis, Imbalanced data classification, Data resampling, Random forest},
abstract = {The problem of imbalanced data classification often exists in medical diagnosis. Traditional classification algorithms usually assume that the number of samples in each class is similar and their misclassification cost during training is equal. However, the misclassification cost of patient samples is higher than that of healthy person samples. Therefore, how to increase the identification of patients without affecting the classification of healthy individuals is an urgent problem. In order to solve the problem of imbalanced data classification in medical diagnosis, we propose a hybrid sampling algorithm called RFMSE, which combines the Misclassification-oriented Synthetic minority over-sampling technique (M-SMOTE) and Edited nearset neighbor (ENN) based on Random forest (RF). The algorithm is mainly composed of three parts. First, M-SMOTE is used to increase the number of samples in the minority class, while the over-sampling rate of M-SMOTE is the misclassification rate of RF. Then, ENN is used to remove the noise ones from the majority samples. Finally, RF is used to perform classification prediction for the samples after hybrid sampling, and the stopping criterion for iterations is determined according to the changes of the classification index (i.e. Matthews Correlation Coefficient (MCC)). When the value of MCC continuously drops, the process of iterations will be stopped. Extensive experiments conducted on ten UCI datasets demonstrate that RFMSE can effectively solve the problem of imbalanced data classification. Compared with traditional algorithms, our method can improve F-value and MCC more effectively.}
}

@INPROCEEDINGS{zeng7563084,
  author={Zeng, Min and Zou, Beiji and Wei, Faran and Liu, Xiyao and Wang, Lei},
  booktitle={2016 IEEE International Conference of Online Analysis and Computing Science (ICOACS)}, 
  title={Effective prediction of three common diseases by combining SMOTE with Tomek links technique for imbalanced medical data}, 
  year={2016},
  volume={},
  number={},
  pages={225-228},
  keywords={Decision support systems;Diabetes;Pathology;Parkinson's disease;imbalanced medical data;SMOTE;Tomek links;diabetes;vertebral column pathologies;Parkinson's disease},
  doi={10.1109/ICOACS.2016.7563084}}

@article{Ramentol2012,
  author    = {Enislay Ramentol and Yailé Caballero and Rafael Bello and Francisco Herrera},
  title     = {SMOTE-RSB*: A hybrid preprocessing approach based on oversampling and undersampling for high imbalanced data-sets using SMOTE and rough sets theory},
  journal   = {Knowledge and Information Systems},
  year      = {2012},
  volume    = {33},
  pages     = {245--265},
  doi       = {10.1007/s10115-011-0465-6}
}
@INPROCEEDINGS{Choirunnisa8864335,
  author={Choirunnisa, Shabrina and Lianto, Joko},
  booktitle={2018 International Seminar on Research of Information Technology and Intelligent Systems (ISRITI)}, 
  title={Hybrid Method of Undersampling and Oversampling for Handling Imbalanced Data}, 
  year={2018},
  volume={},
  number={},
  pages={276-280},
  keywords={Cleaning;Classification algorithms;Training;Decision trees;Data mining;Data models;Imbalance;undersampling;oversampling;NCL;A-SUWO;natural data},
  doi={10.1109/ISRITI.2018.8864335}}

@book{ChakrabortyDey2024,
  author    = {Sanjay Chakraborty and Lopamudra Dey},
  title     = {Multi-objective, Multi-class and Multi-label Data Classification with Class Imbalance: Theory and Practices},
  series    = {Springer Tracts in Nature-Inspired Computing},
  volume    = {XX},
  publisher = {Springer Singapore},
  year      = {2024},
  isbn      = {978-981-97-9621-2 (hc), 978-981-97-9624-3 (sc), 978-981-97-9622-9 (ebook)},
  doi       = {10.1007/978-981-97-9622-9}
}

@online{IBM2025_classificationML,
  author    = {IBM Corporation},
  title     = {What is classification in machine learning?},
  year      = {2025},
  url       = {https://www.ibm.com/think/topics/classification-machine-learning},
  note      = {Accessed: 2025-11-12}
}

@article{Iqbal2023,
  author    = {Saeed Iqbal and Adnan N. Qureshi and Jianqiang Li and Imran Arshad Choudhry and Tariq Mahmood},
  title     = {Dynamic learning for imbalanced data in learning chest X-ray and CT images},
  journal   = {Heliyon},
  year      = {2023},
  volume    = {9},
  number    = {6},
  pages     = {e16807},
  doi       = {10.1016/j.heliyon.2023.e16807},
  publisher = {Elsevier}
}

@article{abdelkhalek2023addressing,
  title={Addressing the class imbalance problem in network intrusion detection systems using data resampling and deep learning.},
  author={Abdelkhalek, Ahmed and Mashaly, Maggie},
  journal={Journal of Supercomputing},
  volume={79},
  number={10},
  year={2023}
}


@Article{cancers16193417,
AUTHOR = {Gurcan, Fatih and Soylu, Ahmet},
TITLE = {Learning from Imbalanced Data: Integration of Advanced Resampling Techniques and Machine Learning Models for Enhanced Cancer Diagnosis and Prognosis},
JOURNAL = {Cancers},
VOLUME = {16},
YEAR = {2024},
NUMBER = {19},
ARTICLE-NUMBER = {3417},
URL = {https://www.mdpi.com/2072-6694/16/19/3417},
PubMedID = {39410036},
ISSN = {2072-6694},
ABSTRACT = {Background/Objectives: This study aims to evaluate the performance of various classification algorithms and resampling methods across multiple diagnostic and prognostic cancer datasets, addressing the challenges of class imbalance. Methods: A total of five datasets were analyzed, including three diagnostic datasets (Wisconsin Breast Cancer Database, Cancer Prediction Dataset, Lung Cancer Detection Dataset) and two prognostic datasets (Seer Breast Cancer Dataset, Differentiated Thyroid Cancer Recurrence Dataset). Nineteen resampling methods from three categories were employed, and ten classifiers from four distinct categories were utilized for comparison. Results: The results demonstrated that hybrid sampling methods, particularly SMOTEENN, achieved the highest mean performance at 98.19%, followed by IHT (97.20%) and RENN (96.48%). In terms of classifiers, Random Forest showed the best performance with a mean value of 94.69%, with Balanced Random Forest and XGBoost following closely. The baseline method (no resampling) yielded a significantly lower performance of 91.33%, highlighting the effectiveness of resampling techniques in improving model outcomes. Conclusions: This research underscores the importance of resampling methods in enhancing classification performance on imbalanced datasets, providing valuable insights for researchers and healthcare professionals. The findings serve as a foundation for future studies aimed at integrating machine learning techniques in cancer diagnosis and prognosis, with recommendations for further research on hybrid models and clinical applications.},
DOI = {10.3390/cancers16193417}
}

@article{mutanov2021multi,
  title={Multi-Class Sentiment Analysis of Social Media Data with Machine Learning Algorithms.},
  author={Mutanov, Galimkair and Karyukin, Vladislav and Mamykova, Zhanl},
  journal={Computers, Materials \& Continua},
  volume={69},
  number={1},
  year={2021}
}

@article{BUDA2018249,
title = {A systematic study of the class imbalance problem in convolutional neural networks},
journal = {Neural Networks},
volume = {106},
pages = {249-259},
year = {2018},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302107},
author = {Mateusz Buda and Atsuto Maki and Maciej A. Mazurowski},
keywords = {Class imbalance, Convolutional neural networks, Deep learning, Image classification},
abstract = {In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.}
}

@ARTICLE{face8708977,
  author={Huang, Chen and Li, Yining and Loy, Chen Change and Tang, Xiaoou},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Deep Imbalanced Learning for Face Recognition and Attribute Prediction}, 
  year={2020},
  volume={42},
  number={11},
  pages={2781-2794},
  keywords={Face;Face recognition;Training;Task analysis;Protocols;Semantics;Image segmentation;Imbalanced learning;deep convolutional neural networks;face recognition;attribute prediction},
  doi={10.1109/TPAMI.2019.2914680}}
@article{SIDUMO2022101822,
title = {An approach to multi-class imbalanced problem in ecology using machine learning},
journal = {Ecological Informatics},
volume = {71},
pages = {101822},
year = {2022},
issn = {1574-9541},
doi = {https://doi.org/10.1016/j.ecoinf.2022.101822},
url = {https://www.sciencedirect.com/science/article/pii/S1574954122002722},
author = {Bonelwa Sidumo and Energy Sonono and Isaac Takaidza},
keywords = {Ecology, Ensemble classifiers, Imbalanced dataset, Machine learning, Multi-class classification, Multiple species},
abstract = {Ecologists collect their data manually by visiting multiple sampling sites. Since there can be multiple species in the multiple sampling sites, manually classifying them can be a daunting task. Much work in literature has focused mostly on statistical methods for classification of single species and very few studies on classification of multiple species. In addition to looking at multiple species, we noted that classification of multiple species result in multi-class imbalanced problem. This study proposes to use machine learning approach to classify multiple species in population ecology. In particular, bagging (random forests (RF) and bagging classification trees (bagCART)) and boosting (boosting classification trees (bootCART), gradient boosting machines (GBM) and adaptive boosting classification trees (AdaBoost)) classifiers were evaluated for their performances on imbalanced multiple fish species dataset. The recall and F1-score performance metrics were used to select the best classifier for the dataset. The bagging classifiers (RF and bagCART) achieved high performances on the imbalanced dataset while the boosting classifiers (bootCART, GBM and AdaBoost) achieved lower performances on the imbalanced dataset. We found that some machine learning classifiers were sensitive to imbalanced dataset hence they require data resampling to improve their performances. After resampling, the bagging classifiers (RF and bagCART) had high performances compared to boosting classifiers (bootCART, GBM and AdaBoost). The strong performances shown by bagging classifiers (RF and bagCART) suggest that they can be used for classifying multiple species in ecological studies.}
}

@article{HAIXIANG2017220,
title = {Learning from class-imbalanced data: Review of methods and applications},
journal = {Expert Systems with Applications},
volume = {73},
pages = {220-239},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.12.035},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416307175},
author = {Guo Haixiang and Li Yijing and Jennifer Shang and Gu Mingyun and Huang Yuanyue and Gong Bing},
keywords = {Rare events, Imbalanced data, Machine learning, Data mining},
abstract = {Rare events, especially those that could potentially negatively impact society, often require humans’ decision-making responses. Detecting rare events can be viewed as a prediction task in data mining and machine learning communities. As these events are rarely observed in daily life, the prediction task suffers from a lack of balanced data. In this paper, we provide an in depth review of rare event detection from an imbalanced learning perspective. Five hundred and seventeen related papers that have been published in the past decade were collected for the study. The initial statistics suggested that rare events detection and imbalanced learning are concerned across a wide range of research areas from management science to engineering. We reviewed all collected papers from both a technical and a practical point of view. Modeling methods discussed include techniques such as data preprocessing, classification algorithms and model evaluation. For applications, we first provide a comprehensive taxonomy of the existing application domains of imbalanced learning, and then we detail the applications for each category. Finally, some suggestions from the reviewed papers are incorporated with our experiences and judgments to offer further research directions for the imbalanced learning and rare event detection fields.}
}

@article{YUAN2018160,
title = {A regularized ensemble framework of deep learning for cancer detection from multi-class, imbalanced training data},
journal = {Pattern Recognition},
volume = {77},
pages = {160-172},
year = {2018},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2017.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0031320317305034},
author = {Xiaohui Yuan and Lijun Xie and Mohamed Abouelenien},
keywords = {Ensemble, Deep learning, Imbalanced data, Cancer detection},
}

@article{CERVANTES2020189,
title = {A comprehensive survey on support vector machine classification: Applications, challenges and trends},
journal = {Neurocomputing},
volume = {408},
pages = {189-215},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.10.118},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220307153},
author = {Jair Cervantes and Farid Garcia-Lamont and Lisbeth Rodríguez-Mazahua and Asdrubal Lopez},
keywords = {SVM, Classification, Machine learning},
abstract = {In recent years, an enormous amount of research has been carried out on support vector machines (SVMs) and their application in several fields of science. SVMs are one of the most powerful and robust classification and regression algorithms in multiple fields of application. The SVM has been playing a significant role in pattern recognition which is an extensively popular and active research area among the researchers. Research in some fields where SVMs do not perform well has spurred development of other applications such as SVM for large data sets, SVM for multi classification and SVM for unbalanced data sets. Further, SVM has been integrated with other advanced methods such as evolve algorithms, to enhance the ability of classification and optimize parameters. SVM algorithms have gained recognition in research and applications in several scientific and engineering areas. This paper provides a brief introduction of SVMs, describes many applications and summarizes challenges and trends. Furthermore, limitations of SVMs will be identified. The future of SVMs will be discussed in conjunction with further applications. The applications of SVMs will be reviewed as well, especially in the some fields.}
}

@online{MathWorks_SVM_image,
  author    = {MathWorks, Inc.},
  title     = {Support Vector Machines for Binary Classification — Figure illustration},
  year      = {2025},
  url       = {https://www.mathworks.com/help/stats/support-vector-machines-for-binary-classification.html},
  note      = {Figure from MathWorks documentation, accessed: 2025-11-17}
}

@article{burges1998tutorial,
  title={A tutorial on support vector machines for pattern recognition},
  author={Burges, Christopher JC},
  journal={Data mining and knowledge discovery},
  volume={2},
  number={2},
  pages={121--167},
  year={1998},
  publisher={Springer}
}

@online{GeeksforGeeks_SVM,
  author    = {GeeksforGeeks},
  title     = {Support Vector Machine (SVM) Algorithm},
  year      = {2025},
  url       = {https://www.geeksforgeeks.org/support-vector-machine-algorithm/},
  note      = {Last updated: 13 Nov, 2025; Accessed: 2025-11-17}
}

@article{debnath2004decision,
  title={A decision based one-against-one method for multi-class support vector machine},
  author={Debnath, Rameswar and Takahide, Nogayama and Takahashi, Haruhisa},
  journal={Pattern Analysis and Applications},
  volume={7},
  number={2},
  pages={164--175},
  year={2004},
  publisher={Springer}
}

@ARTICLE{Wei991427,
  author={Chih-Wei Hsu and Chih-Jen Lin},
  journal={IEEE Transactions on Neural Networks}, 
  title={A comparison of methods for multiclass support vector machines}, 
  year={2002},
  volume={13},
  number={2},
  pages={415-425},
  keywords={Support vector machines;Support vector machine classification;Optimization methods;Large-scale systems;Computer science;Training data},
  doi={10.1109/72.991427}
}

@article{neelamegam2013classification,
  title={Classification algorithm in data mining: An overview},
  author={Neelamegam, S and Ramaraj, E},
  journal={International Journal of P2P Network Trends and Technology (IJPTT)},
  volume={4},
  number={8},
  pages={369--374},
  year={2013}
}

@article{Charbuty_Abdulazeez_2021, title={Classification Based on Decision Tree Algorithm for Machine Learning}, volume={2}, url={https://jastt.org/index.php/jasttpath/article/view/65}, DOI={10.38094/jastt20165}, abstractNote={
Decision tree classifiers are regarded to be a standout of the most well-known methods to data classification representation of classifiers. Different researchers from various fields and backgrounds have considered the problem of extending a decision tree from available data, such as machine study, pattern recognition, and statistics. In various fields such as medical disease analysis, text classification, user smartphone classification, images, and many more the employment of Decision tree classifiers has been proposed in many ways. This paper provides a detailed approach to the decision trees. Furthermore, paper specifics, such as algorithms/approaches used, datasets, and outcomes achieved, are evaluated and outlined comprehensively. In addition, all of the approaches analyzed were discussed to illustrate the themes of the authors and identify the most accurate classifiers. As a result, the uses of different types of datasets are discussed and their findings are analyzed.
}, number={01}, journal={Journal of Applied Science and Technology Trends}, author={Charbuty, Bahzad and Abdulazeez, Adnan}, year={2021}, month={Mar.}, pages={20–28} }

@article{saputra2023,
author = {Dwifana Saputra, Adi and Hindarto, Djarot and Haryono, Haryono},
year = {2023},
month = {01},
pages = {157-165},
title = {Supervised Learning from Data Mining on Process Data Loggers on Micro-Controllers},
volume = {8},
journal = {Sinkron},
doi = {10.33395/sinkron.v8i1.11942}
}

@article{kotsiantis2013decision,
  title={Decision trees: a recent overview},
  author={Kotsiantis, Sotiris B},
  journal={Artificial Intelligence Review},
  volume={39},
  number={4},
  pages={261--283},
  year={2013},
  publisher={Springer}
}

@article{priyam2013comparative,
  title={Comparative analysis of decision tree classification algorithms},
  author={Priyam, Anuja and Abhijeeta, Gupta R and Rathee, Anju and Srivastava, Saurabh},
  journal={International Journal of current engineering and technology},
  volume={3},
  number={2},
  pages={334--337},
  year={2013}
}

@article{farris2010gini,
  title={The Gini index and measures of inequality},
  author={Farris, Frank A},
  journal={The American Mathematical Monthly},
  volume={117},
  number={10},
  pages={851--864},
  year={2010},
  publisher={Taylor \& Francis}
}

@INPROCEEDINGS{Somvanshi7860040,
  author={Somvanshi, Madan and Chavan, Pranjali and Tambade, Shital and Shinde, S. V.},
  booktitle={2016 International Conference on Computing Communication Control and automation (ICCUBEA)}, 
  title={A review of machine learning techniques using decision tree and support vector machine}, 
  year={2016},
  volume={},
  number={},
  pages={1-7},
  keywords={Decision trees;Classification algorithms;Support vector machines;Machine learning algorithms;Data mining;Databases;Supervised learning;classification;machine learning;decision tree;id3;support vector machine;kernel},
  doi={10.1109/ICCUBEA.2016.7860040}}

@INPROCEEDINGS{Pathak9034296,
  author={Pathak, Soham and Mishra, Indivar and Swetapadma, Aleena},
  booktitle={2018 3rd International Conference on Inventive Computation Technologies (ICICT)}, 
  title={An Assessment of Decision Tree based Classification and Regression Algorithms}, 
  year={2018},
  volume={},
  number={},
  pages={92-95},
  keywords={Vegetation;Regression tree analysis;Classification algorithms;Prediction algorithms;Inference algorithms;Mathematical model;Decision Tree;Classification;Machine Learning;Regression;Information gain},
  doi={10.1109/ICICT43934.2018.9034296}}

@article{salman2024random,
  title={Random forest algorithm overview},
  author={Salman, Hasan Ahmed and Kalakech, Ali and Steiti, Amani},
  journal={Babylonian Journal of Machine Learning},
  volume={2024},
  pages={69--79},
  year={2024}
}

@article{kulkarni2013random,
  title={Random forest classifiers: a survey and future research directions},
  author={Kulkarni, Vrushali Y and Sinha, Pradeep K},
  journal={Int. J. Adv. Comput},
  volume={36},
  number={1},
  pages={1144--1153},
  year={2013}
}

@article{song2015decision,
  title={Decision tree methods: applications for classification and prediction},
  author={Song, Yan-Yan and Lu, Ying},
  journal={Shanghai archives of psychiatry},
  year={2015},
  publisher={Shanghai Municipal Bureau of Publishing}
}

@article{islam2019overview,
  title={An overview of neural network},
  author={Islam, Mohaiminul and Chen, Guorong and Jin, Shangzhu},
  journal={American Journal of Neural Networks and Applications},
  volume={5},
  number={1},
  pages={7--11},
  year={2019},
  publisher={Science Publishing Group}
}

@article{abiodun2018state,
  title={State-of-the-art in artificial neural network applications: A survey},
  author={Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun Esther and Dada, Kemi Victoria and Mohamed, Nachaat AbdElatif and Arshad, Humaira},
  journal={Heliyon},
  volume={4},
  number={11},
  year={2018},
  publisher={Elsevier}
}

@online{MathWorks_CNN_DiscoveryFig,
  author    = {MathWorks, Inc.},
  title     = {Illustration of a Convolutional Neural Network},
  year      = {2025},
  url       = {https://www.mathworks.com/discovery/convolutional-neural-network.html},
  note      = {Accessed: 2025-11-19}
}

@article{qamar2023artificial,
  title={Artificial neural networks: An overview},
  author={Qamar, Roheen and Zardari, Baqar Ali},
  journal={Mesopotamian Journal of Computer Science},
  volume={2023},
  pages={124--133},
  year={2023}
}

@article{chen2021review,
  title={Review of image classification algorithms based on convolutional neural networks},
  author={Chen, Leiyu and Li, Shaobo and Bai, Qiang and Yang, Jing and Jiang, Sanlong and Miao, Yanming},
  journal={Remote Sensing},
  volume={13},
  number={22},
  pages={4712},
  year={2021},
  publisher={MDPI}
}

@online{Jorgecardete2024_CNNGuide,
  author    = {Jorgecardete},
  title     = {Convolutional Neural Networks: A Comprehensive Guide},
  journal   = {Medium – The Deep Hub},
  year      = {2024},
  month     = {February},
  url       = {https://medium.com/thedeephub/convolutional-neural-networks-a-comprehensive-guide-5cc0b5eae175},
  note      = {Accessed: 2025-11-20}
}

@article{sakib2019overview,
  title={An overview of convolutional neural network: Its architecture and applications},
  author={Sakib, Shadman and Ahmed, Nazib and Kabir, Ahmed Jawad and Ahmed, Hridon},
  year={2019},
  publisher={Preprints}
}
